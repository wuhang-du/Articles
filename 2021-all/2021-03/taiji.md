# Taiji: Managing Global User Traffic for Large-Scale Internet Services at the Edge  

意图解决的问题：
1. 容量短缺
2. 产品异构：新功能
3. 硬件异构：硬件的改动
4. 错误容忍：容错   
   
本质上：一个分配问题：生成路由表；调度？
静态内容是边缘能处理，如果是动态的内容，如何处理？  
  

文章主体：  
1. 架构：
```go

client-req 
  |
edges --- > cluster1，cluster2
  |
taiji

taiji主要由运行时与流量管道组成：
1.运行时：根据需求与反馈（边缘流量负载，边缘与数据中心延迟）与资源状态（数据中心利用率）+ 服务级别的策略 生成路由表。
2.管道：将路由表翻译成细粒度的规则，利用关系感知路由为边缘节点生成最终的路由配置？
```

2. 运行时  
无状态请求用qps衡量，有状态使用连接数衡量。  
在两个数据中心之间切换一单位的流量、识别当前最优的路由策略、不断迭代直到不能实现更好的结果。  
控制频率以防引起抖动。  
这步实际上生成的是整体的一个调度。  
```go
edge1: dc1:50%,dc2:40%,dc3:10%
edge2: dc1:30%,dc2:50%,dc3:20% 
```

3. 流量管道  
在运行时的基础上，将edge1的流量再次细分到用户的层面。  
其实：这是一个变向的汇聚；对后端更有利一些。  
  
那如何生成汇聚关系：关系感知。访问相同内容的用户流量具备局部性。 
  
多层，多段；先在层的粒度去分，不行的话，在桶的粒度去分配。
桶太小：可能把大的社区给分割了，影响缓存。
桶太大：影响调整的精度。

```go
离线任务：用户与桶的维护与更新。Social Hash，周粒度。
在线任务：桶与数据中心的分配
```  

4. 边缘流量转发
边缘直接存储用户到数据中心的关系，数据量太大了。  
边缘存着桶与数据中心路由；数据中心存储用户与数据中心路由；
用户第一次就近去数据中心拿桶信息，后续边缘按路由分配。  

5. 总结：
一个两层调度：运行时算有边缘与机房的分配；管道算好边缘内各用户的与机房的分配。